# ğŸ‘— Fashion-MNIST Classification â€” MLOps Project

## ğŸš€ Project Overview

This project implements a **reproducible end-to-end machine learning pipeline** for image classification, with a strong emphasis on **MLOps best practices** rather than achieving state-of-the-art accuracy.

The goal is to demonstrate how a real-world machine learning system can be:
- cleanly structured  
- configuration-driven  
- fully reproducible  
- experiment-tracked  
- containerized and deployable  

We use the **Fashion-MNIST** dataset to classify grayscale images of clothing items. Each image is `28Ã—28` pixels and belongs to one of **10 clothing categories** (e.g. T-shirt/top, trouser, sneaker, coat).

ğŸ“¦ **Dataset size:** 70,000 images  
ğŸ§µ **Classes:** 10 clothing types  
ğŸ“ **Versioning:** DVC  

ğŸ”— Dataset link:  
https://www.kaggle.com/datasets/zalando-research/fashionmnist

---

## ğŸ§  What This Project Demonstrates

### ğŸ“ Application Logging (M14)
- Structured logging using **Loguru**
- Logs important runtime events:
  - device selection (CPU / MPS / CUDA)
  - training start and completion
  - batch- and epoch-level progress
  - evaluation metrics
  - model checkpoint saving
- Makes debugging and monitoring easier and more transparent

---

### ğŸ“Š Experiment Tracking with Weights & Biases (M14)
- Logs:
  - training loss and accuracy
  - test accuracy per epoch
  - training curves as plots
  - trained models as **W&B artifacts**
- Stores full experiment configuration automatically
- Enables easy comparison and reproducibility via the W&B dashboard

---

### ğŸ” Hyperparameter Optimization (M14)
- Hyperparameter sweeps using **Weights & Biases Sweeps**
- Tuned parameters:
  - learning rate
  - batch size
  - number of epochs
  - model type (MLP vs CNN)
- Optimization metric:
  - `epoch/test_acc`
- Multiple experiments launched automatically using sweep agents

---

## ğŸ—‚ Project Structure

```txt
â”œâ”€â”€ .github/                  # CI and GitHub actions
â”‚   â”œâ”€â”€ dependabot.yaml
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ tests.yaml
â”œâ”€â”€ configs/                  # Hydra & W&B configuration files
â”‚   â””â”€â”€ sweep.yaml
â”œâ”€â”€ data/                     # Versioned data (DVC)
â”‚   â”œâ”€â”€ processed/
â”‚   â””â”€â”€ raw/
â”œâ”€â”€ dockerfiles/              # Dockerfiles
â”‚   â”œâ”€â”€ api.Dockerfile
â”‚   â””â”€â”€ train.Dockerfile
â”œâ”€â”€ models/                   # Trained model checkpoints
â”œâ”€â”€ notebooks/                # Exploration notebooks
â”œâ”€â”€ reports/
â”‚   â””â”€â”€ figures/              # Training plots
â”œâ”€â”€ src/
â”‚   â””â”€â”€ fashionmnist_classification_mlops/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ api.py
â”‚       â”œâ”€â”€ data.py
â”‚       â”œâ”€â”€ evaluate.py
â”‚       â”œâ”€â”€ logging_utils.py
â”‚       â”œâ”€â”€ model.py
â”‚       â”œâ”€â”€ sweep_runner.py
â”‚       â”œâ”€â”€ train.py
â”‚       â””â”€â”€ visualize.py
â”œâ”€â”€ tests/                    # Unit tests
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .pre-commit-config.yaml
â”œâ”€â”€ LICENSE
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â””â”€â”€ requirements_dev.txt
```



# âš™ï¸ Setup Instructions (From Scratch)
## 1ï¸âƒ£ Clone the Repository

```bash
git clone <repository-url>
cd MLOPS-Project
```

## 2ï¸âƒ£ Create and Activate Virtual Environment
````bash
python -m venv .venv
source .venv/bin/activate
````

## 3ï¸âƒ£ Install Dependencies
````bash
pip install -r requirements.txt
pip install -r requirements_dev.txt
````

## 4ï¸âƒ£ Install Project as Editable Package
````bash
pip install -e .
````

## 5ï¸âƒ£ Add DVC Configuration for Google Drive Remote
We are using Google Drive as the remote storage for DVC, ensure the .dvc/config file is correctly set up. Add the following configuration:

```
[core]
    remote = gdrive_remote

[remote "gdrive_remote"]
    url = gdrive://<your-drive-folder-id>
    gdrive_client_id = "<your-client-id>"
    gdrive_client_secret = "<your-client-secret>"
```
# ğŸ“¦ Data Version Control (DVC)

> Prerequisite: Docker installed and running

Install DVC and remote storage support:

````bash
pip install dvc dvc-gdrive
````
Pull the versioned dataset:

````bash
dvc pull
````

This populates:

- data/raw/

- data/processed/

# ğŸ‹ï¸ Training the Model (Local Python)
Default training:

````bash
python -m fashionmnist_classification_mlops.train

Custom hyperparameters:
python -m fashionmnist_classification_mlops.train \
  hyperparameters.learning_rate=0.001 \
  hyperparameters.batch_size=64 \
  hyperparameters.epochs=5 \
 ````

# ğŸ“Œ Training automatically logs metrics, plots, and models.

## ğŸ” Hyperparameter Sweeps (Weights & Biases)
### 1ï¸âƒ£ Login to W&B
````bash
wandb login
````

### 2ï¸âƒ£ Create a Sweep
````bash
wandb sweep configs/sweep.yaml
````

This command returns a sweep ID.

### 3ï¸âƒ£ Run the Sweep Agent
```bash
wandb agent <entity>/<project>/<sweep_id>
```

The agent automatically:

- launches multiple training runs

- explores different hyperparameter combinations

- logs everything to the W&B dashboard

# ğŸ³ Dockerized Execution
Train the Model in Docker:

```bash
docker build -f dockerfiles/train.Dockerfile -t fashionmnist-train .
docker run --rm fashionmnist-train
```

Run the Inference API:
```bash
docker build -f dockerfiles/api.Dockerfile -t fashionmnist-api .
docker run -p 8000:8000 fashionmnist-api
```
ğŸš€ The API exposes an endpoint for Fashion-MNIST predictions.

- â™»ï¸ Reproducibility

Every experiment can be reproduced using:

- Git commit hash

- Hydra configuration files

- DVC-tracked data versions

- Logged hyperparameters

- W&B artifacts and run metadata

This ensures full traceability from raw data to trained model.